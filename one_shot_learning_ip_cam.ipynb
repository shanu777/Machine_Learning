{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os import listdir\n",
    "import cv2\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "detector=MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = (67,67,67)\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    #preprocess_input normalizes input in scale of [-1, +1]. You must apply same normalization in prediction.\n",
    "    #Ref: https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py (Line 45)\n",
    "    img = preprocess_input(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadVggFaceModel():\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "\tmodel.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "\tmodel.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Convolution2D(2622, (1, 1)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Activation('softmax'))\n",
    "\t\n",
    "\t#you can download pretrained weights from https://drive.google.com/file/d/1CPSeum3HpopfomUEK1gybeuIVoeJT_Eo/view?usp=sharing\n",
    "\tfrom keras.models import model_from_json\n",
    "\tmodel.load_weights('vgg_face_weights.h5')\n",
    "\t\n",
    "\tvgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)\n",
    "\t\n",
    "\treturn vgg_face_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = loadVggFaceModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_images = \"D:/data/oneshot/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = dict()\n",
    "\n",
    "for file in listdir(person_images):\n",
    "\tperson, extension = file.split(\".\")\n",
    "\tpersons[person] = model.predict(preprocess_image('D:/data/oneshot/%s.jpg' % (person)))[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCosineSimilarity(source_representation, test_representation):\n",
    "    a = np.matmul(np.transpose(source_representation), test_representation)\n",
    "    b = np.sum(np.multiply(source_representation, source_representation))\n",
    "    c = np.sum(np.multiply(test_representation, test_representation))\n",
    "    return 1 - (a / (np.sqrt(b) * np.sqrt(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Capture time duration of faces detected\n",
    "faceFound = {}\n",
    "ts = [0,0,False,' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib import request as ur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='http://192.168.1.225:8080/out.jpg?q=30&id=0.5380787588953155&r=1579594544773'\n",
    "while True:\n",
    "    imgresp=ur.urlopen(url)\n",
    "    imgNp=np.array(bytearray(imgresp.read()),dtype=np.uint8)\n",
    "    img=cv2.imdecode(imgNp,-1)\n",
    "    #_,img=video_capture.read()\n",
    "\n",
    "    result=detector.detect_faces(img)\n",
    "# in the result we will have multiple number of dictionaries for each image, and we need to extract the bounding box of the face.      \n",
    "    if len(result):\n",
    "        if ts[2] == False:\n",
    "            ts[0] = datetime.now()\n",
    "            ts[2] = True\n",
    "        for x in result:\n",
    "            #print (result)\n",
    "            bounding_box=x['box']\n",
    "\n",
    "            #after extracting bounding box, draw rectangle around the face.\n",
    "\n",
    "            cv2.rectangle(img,(bounding_box),(0,0,255),3)\n",
    "\n",
    "            #save the image.\n",
    "\n",
    "            x1,y3,w,h=x['box']\n",
    "            x2,y2=x1+w,y3+h\n",
    "            cropped=img[y3:y2, x1:x2]\n",
    "            cropped = cv2.resize(cropped, (224, 224))\n",
    "            img_pixels = image.img_to_array(cropped)\n",
    "            img_pixels = np.expand_dims(img_pixels, axis = 0)\n",
    "    #img_pixels /= 255\n",
    "    #employee dictionary is using preprocess_image and it normalizes in scale of [-1, +1]\n",
    "            img_pixels /= 127.5\n",
    "            img_pixels -= 1\n",
    "\n",
    "            captured_representation = model.predict(img_pixels)[0,:]\n",
    "            #print(captured_representation)\n",
    "\n",
    "            found = 0\n",
    "            for i in persons:\n",
    "                person_name = i\n",
    "                representation = persons[i]\n",
    "                similarity = findCosineSimilarity(representation, captured_representation)\n",
    "                if(similarity < 0.28):\n",
    "                    cv2.putText(img,person_name,(x1,y3-10),cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,255),2,cv2.LINE_AA)\n",
    "                    ts[3] = person_name\n",
    "\n",
    "                    found = 1\n",
    "                    break\n",
    "\n",
    "    #connect face and text\n",
    "            #cv2.line(img,(int((x+x+w)/2),y+15),(x+w,y-20),color,1)\n",
    "            #cv2.line(img,(x+w,y-20),(x+w+10,y-20),color,1)\n",
    "\n",
    "            if(found == 0): #if found image is not in employee database\n",
    "                cv2.putText(img,' ',(x1,y3-10),cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,255),2,cv2.LINE_AA)\n",
    "    else:\n",
    "        if ts[2] == True:\n",
    "            ts[1] = datetime.now()\n",
    "            ts[2] = False\n",
    "            diff = ts[1]-ts[0]\n",
    "            if diff.seconds!=0:\n",
    "                faceFound.update({ts[3]:[diff.seconds+1, ts[0].strftime(\"%m/%d/%Y %H:%M:%S\"), ts[1].strftime(\"%m/%d/%Y %H:%M:%S\")]})\n",
    "\n",
    "\n",
    "    #cv2.imshow('img',img)\n",
    "\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'): #press q to quit\n",
    "        break\n",
    "\n",
    "    #kill open cv things\t\t\n",
    "    #cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_log=pd.DataFrame(faceFound,index=('total time(sec)','entry time','exit time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shanu</th>\n",
       "      <th>unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total time(sec)</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entry time</th>\n",
       "      <td>01/21/2020 15:14:17</td>\n",
       "      <td>01/21/2020 15:14:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exit time</th>\n",
       "      <td>01/21/2020 15:14:20</td>\n",
       "      <td>01/21/2020 15:14:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               shanu               unique\n",
       "total time(sec)                    3                    7\n",
       "entry time       01/21/2020 15:14:17  01/21/2020 15:14:10\n",
       "exit time        01/21/2020 15:14:20  01/21/2020 15:14:16"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
